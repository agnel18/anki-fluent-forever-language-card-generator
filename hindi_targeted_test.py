#!/usr/bin/env python3
"""
Hindi Analyzer Targeted Edge Case Testing
Focuses on failures and high-impact edge cases from frequency list
"""

import sys
import time
import json
import os
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from streamlit_app.language_analyzers.analyzers.hi_analyzer import HiAnalyzer

def test_edge_cases():
    """Test specific edge cases identified from QA testing"""

    # High-priority edge cases from current failures
    edge_cases = {
        'postposition_failures': [
            {'word': '‡§ï‡•ã', 'sentence': '‡§µ‡§π ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§ï‡•ã ‡§ó‡§Ø‡§æ‡•§', 'expected': 'postposition', 'color': '#4444FF'},  # This one works
            {'word': '‡§â‡§∏‡•á', 'sentence': '‡§Æ‡•à‡§Ç ‡§â‡§∏‡•á ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§¶‡•á‡§§‡§æ ‡§π‡•Ç‡§Ç‡•§', 'expected': 'postposition', 'color': '#4444FF'},  # Fixed: ‡§â‡§∏‡•á is the postposition here
        ],
        'homonyms_and_context': [
            {'word': '‡§ñ‡§æ‡§®‡§æ', 'sentence': '‡§Æ‡•à‡§Ç ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ‡§§‡§æ ‡§π‡•Ç‡§Ç‡•§', 'expected': 'verb', 'color': '#44FF44'},  # As verb
            {'word': '‡§ñ‡§æ‡§®‡§æ', 'sentence': '‡§ñ‡§æ‡§®‡§æ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡•§', 'expected': 'noun', 'color': '#FFAA00'},    # As noun
            {'word': '‡§ï‡§æ‡§Æ', 'sentence': '‡§Æ‡•à‡§Ç ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•Ç‡§Ç‡•§', 'expected': 'noun', 'color': '#FFAA00'},   # Work as noun
            {'word': '‡§ï‡§æ‡§Æ', 'sentence': '‡§Ø‡§π ‡§ï‡§æ‡§Æ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§§‡§æ‡•§', 'expected': 'verb', 'color': '#44FF44'}, # Work as verb
        ],
        'complex_particles': [
            {'word': '‡§§‡§•‡§æ', 'sentence': '‡§∞‡§æ‡§Æ ‡§§‡§•‡§æ ‡§∂‡•ç‡§Ø‡§æ‡§Æ ‡§Ü‡§è‡•§', 'expected': 'conjunction', 'color': '#888888'},
            {'word': '‡§Ø‡§æ', 'sentence': '‡§ö‡§æ‡§Ø ‡§Ø‡§æ ‡§ï‡•â‡§´‡•Ä?', 'expected': 'conjunction', 'color': '#888888'},
            {'word': '‡§≤‡•á‡§ï‡§ø‡§®', 'sentence': '‡§µ‡§π ‡§Ü‡§Ø‡§æ ‡§≤‡•á‡§ï‡§ø‡§® ‡§¶‡•á‡§∞ ‡§∏‡•á‡•§', 'expected': 'conjunction', 'color': '#888888'},
        ],
        'high_frequency_challenges': [
            {'word': '‡§∏‡§æ‡§•', 'sentence': '‡§µ‡§π ‡§Æ‡•á‡§∞‡•á ‡§∏‡§æ‡§• ‡§Ü‡§Ø‡§æ‡•§', 'expected': 'postposition', 'color': '#4444FF'},
            {'word': '‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ', 'sentence': '‡§™‡•Å‡§≤‡§ø‡§∏ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ó‡§ø‡§∞‡§´‡•ç‡§§‡§æ‡§∞‡•§', 'expected': 'postposition', 'color': '#4444FF'},
            {'word': '‡§™‡•ç‡§∞‡§§‡§ø', 'sentence': '‡§¶‡§∏ ‡§∞‡•Å‡§™‡§Ø‡•á ‡§™‡•ç‡§∞‡§§‡§ø ‡§ï‡§ø‡§≤‡•ã‡•§', 'expected': 'postposition', 'color': '#4444FF'},
        ]
    }

    return edge_cases

def test_frequency_list_high_impact():
    """Test high-frequency words most likely to cause issues"""

    # Top frequency words that are challenging to classify
    high_impact_words = [
        # Postpositions that might be misclassified
        {'word': '‡§ï‡§æ', 'category': 'postposition', 'sentence': '‡§∞‡§æ‡§Æ ‡§ï‡§æ ‡§ò‡§∞‡•§'},
        {'word': '‡§ï‡•Ä', 'category': 'postposition', 'sentence': '‡§∏‡•Ä‡§§‡§æ ‡§ï‡•Ä ‡§ï‡§ø‡§§‡§æ‡§¨‡•§'},
        {'word': '‡§ï‡•á', 'category': 'postposition', 'sentence': '‡§≤‡§°‡§º‡§ï‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è‡•§'},

        # Words that can be multiple parts of speech
        {'word': '‡§ú‡•à‡§∏‡§æ', 'category': 'conjunction', 'sentence': '‡§ú‡•à‡§∏‡§æ ‡§Ü‡§™ ‡§ï‡§π‡•á‡§Ç‡•§'},
        {'word': '‡§ú‡§æ‡§®‡§®‡§æ', 'category': 'verb', 'sentence': '‡§Æ‡•à‡§Ç ‡§ú‡§æ‡§®‡§§‡§æ ‡§π‡•Ç‡§Ç‡•§'},
        {'word': '‡§¨‡§®‡§æ‡§®‡§æ', 'category': 'verb', 'sentence': '‡§∞‡•ã‡§ü‡•Ä ‡§¨‡§®‡§æ‡§ì‡•§'},

        # Complex particles
        {'word': '‡§Ö‡§ó‡§∞', 'category': 'conjunction', 'sentence': '‡§Ö‡§ó‡§∞ ‡§¨‡§æ‡§∞‡§ø‡§∂ ‡§π‡•ã ‡§§‡•ã‡•§'},
        {'word': '‡§á‡§∏‡§≤‡§ø‡§è', 'category': 'conjunction', 'sentence': '‡§µ‡§π ‡§¨‡•Ä‡§Æ‡§æ‡§∞ ‡§•‡§æ ‡§á‡§∏‡§≤‡§ø‡§è ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§Ø‡§æ‡•§'},
    ]

    return high_impact_words

def run_targeted_test(api_key, test_type="edge_cases"):
    """Run targeted testing with minimal API calls"""

    analyzer = HiAnalyzer()
    results = []
    total_tests = 0
    correct_classifications = 0
    correct_colors = 0

    if test_type == "edge_cases":
        test_cases = test_edge_cases()
        print("üéØ Testing Edge Cases from QA Failures")
        print("=" * 50)

        for category, cases in test_cases.items():
            print(f"\nüìÇ {category.upper().replace('_', ' ')} ({len(cases)} cases)")
            print("-" * 40)

            for case in cases:
                total_tests += 1
                word = case['word']
                sentence = case['sentence']
                expected_role = case['expected']
                expected_color = case['color']

                print(f"Testing: {word} | Expected: {expected_role}")
                print(f"  Sentence: {sentence}")

                result = test_single_word(word, analyzer, api_key, sentence)
                results.append(result)

                if result['success']:
                    actual_role = result['grammatical_role']
                    actual_color = result['color']

                    role_correct = actual_role == expected_role
                    color_correct = actual_color == expected_color

                    if role_correct:
                        correct_classifications += 1
                        print("  ‚úÖ Role correct")
                    else:
                        print(f"  ‚ùå Role wrong: got {actual_role}")

                    if color_correct:
                        correct_colors += 1
                        print("  ‚úÖ Color correct")
                    else:
                        print(f"  ‚ùå Color wrong: got {actual_color}")

                else:
                    print(f"  ‚ùå Failed: {result.get('error', 'Unknown error')}")

                time.sleep(2)  # Rate limiting

    elif test_type == "frequency":
        test_cases = test_frequency_list_high_impact()
        print("üìä Testing High-Impact Frequency Words")
        print("=" * 50)

        for case in test_cases:
            total_tests += 1
            word = case['word']
            sentence = case['sentence']
            expected_category = case['category']

            print(f"Testing: {word} | Category: {expected_category}")
            print(f"  Sentence: {sentence}")

            result = test_single_word(word, analyzer, api_key, sentence)
            results.append(result)

            if result['success']:
                actual_role = result['grammatical_role']
                print(f"  Result: {actual_role} | Color: {result['color']}")

                # For frequency testing, we're more lenient - just check if reasonable
                if actual_role in ['postposition', 'verb', 'conjunction', 'noun', 'adverb']:
                    correct_classifications += 1
                    print("  ‚úÖ Reasonable classification")
                else:
                    print("  ‚ö†Ô∏è Unexpected classification")
            else:
                print(f"  ‚ùå Failed: {result.get('error', 'Unknown error')}")

            time.sleep(2)

    # Summary
    print("\n" + "=" * 50)
    print("üìä TARGETED TEST RESULTS")
    print("=" * 50)

    accuracy = (correct_classifications / total_tests) * 100 if total_tests > 0 else 0
    color_accuracy = (correct_colors / total_tests) * 100 if total_tests > 0 else 0

    print(f"Total targeted tests: {total_tests}")
    print(".1f")
    if test_type == "edge_cases":
        print(".1f")

    # Save results
    filename = f'hindi_targeted_test_{test_type}_results.json'
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump({
            'test_type': test_type,
            'summary': {
                'total_tests': total_tests,
                'accuracy': accuracy,
                'color_accuracy': color_accuracy if test_type == "edge_cases" else None,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            },
            'detailed_results': results
        }, f, ensure_ascii=False, indent=2)

    print(f"\nüìÑ Results saved to {filename}")

    return accuracy, results

def test_single_word(word, analyzer, api_key, sentence=None):
    """Test a single word with custom sentence"""
    try:
        if sentence is None:
            sentence = f"‡§Ø‡§π {word} ‡§π‡•à‡•§"

        result = analyzer.analyze_grammar(
            sentence=sentence,
            target_word=word,
            complexity="beginner",
            groq_api_key=api_key
        )

        # Check confidence score
        if result.confidence_score < 0.85:
            return {
                'word': word,
                'sentence': sentence,
                'error': f'Analysis confidence too low: {result.confidence_score}',
                'success': False
            }

        # Find the target word in grammatical_elements
        word_data = None
        actual_role = None
        actual_color = None

        for role, words in result.grammatical_elements.items():
            for word_info in words:
                if isinstance(word_info, dict) and word_info.get('word') == word:
                    word_data = word_info
                    actual_role = role
                    # Get color from color scheme
                    category = analyzer._map_grammatical_role_to_category(role) if hasattr(analyzer, '_map_grammatical_role_to_category') else role.lower()
                    actual_color = result.color_scheme.get(category, '#888888')
                    break
            if word_data:
                break

        if word_data:
            explanation = result.explanations.get(word, f"{actual_role}")
            return {
                'word': word,
                'sentence': sentence,
                'grammatical_role': actual_role,
                'color': actual_color,
                'explanation': explanation,
                'confidence': result.confidence_score,
                'success': True
            }
        else:
            return {
                'word': word,
                'sentence': sentence,
                'error': 'Word not found in grammatical elements',
                'confidence': result.confidence_score,
                'success': False
            }

    except Exception as e:
        return {
            'word': word,
            'sentence': sentence if 'sentence' in locals() else f"‡§Ø‡§π {word} ‡§π‡•à‡•§",
            'error': str(e),
            'success': False
        }

def main():
    # Get API key
    api_key = os.getenv('GROQ_API_KEY', '').strip()
    if not api_key:
        api_key = input("Enter Groq API key: ").strip()
    if not api_key:
        print("‚ùå No API key provided")
        return

    print("üéØ Hindi Analyzer Targeted Testing Strategy")
    print("=" * 50)
    print("Phase 1: Edge Cases from QA Failures")
    print("Phase 2: High-Impact Frequency Words")
    print()

    # Phase 1: Edge cases
    print("Phase 1: Testing Edge Cases...")
    accuracy1, results1 = run_targeted_test(api_key, "edge_cases")

    # Phase 2: Frequency words (only if Phase 1 passes)
    if accuracy1 >= 80:
        print("\nPhase 2: Testing High-Impact Frequency Words...")
        accuracy2, results2 = run_targeted_test(api_key, "frequency")
    else:
        print("\n‚ö†Ô∏è Phase 1 accuracy below 80%, skipping Phase 2")
        accuracy2 = 0
        results2 = []

    # Final assessment
    print("\n" + "=" * 60)
    print("üéØ TARGETED TESTING COMPLETE")
    print("=" * 60)

    overall_accuracy = (accuracy1 + accuracy2) / 2 if accuracy2 > 0 else accuracy1

    print(".1f")
    print(f"Phase 1 (Edge Cases): {accuracy1:.1f}%")
    if accuracy2 > 0:
        print(f"Phase 2 (Frequency): {accuracy2:.1f}%")

    if overall_accuracy >= 85:
        print("‚úÖ PASSED: Targeted testing successful")
    else:
        print("‚ùå NEEDS IMPROVEMENT: Further analyzer refinement required")

if __name__ == "__main__":
    main()